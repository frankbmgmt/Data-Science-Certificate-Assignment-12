---
title: "Logistic Regression"
author: "Francesco Bonifazi"
date: "7/8/2019"
output: html_document
---

```{r}
install.packages("tidyverse")
install.packages("modelr")
install.packages("caret")
install.packages("titanic")
install.packages("e1071")
```

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("modelr")
library("caret")
library("e1071")
```

# Logistic regression (Again!)

Previously, you used logistic regression on the `titanic_train` data set. This was to familiarize you with adding a classification model to your data. This time, you will tune your model and defend your decisions.

Use logistic regression to predict `Survived` from `titanic_train` data set
```{r}
# Use train data as all data (need Survived column for testing)
raw_dat = titanic::titanic_train

dat = raw_dat %>% 
  mutate(Embarked = if_else(Embarked == '', 'S', Embarked),
         Age = if_else(Age == '' | is.na(Age), mean(Age, na.rm = TRUE), Age),
         Fare = if_else(is.na(Fare), mean(Fare, na.rm = TRUE), Fare),
         Survived = as.factor(Survived)) %>%
  select(-Ticket, -Name, -Cabin) %>%
  as_tibble()
```

# "By Hand"
```{r}
set.seed(123)
training_split = 0.75 #75% data for train, 25% for test
smp_size = floor(training_split * nrow(dat))
train_ind = sample(seq_len(nrow(dat)), size = smp_size)

train_dat = dat[train_ind,]
test_dat = dat[-train_ind,]
```

# "Tidy:

```{r}
set.seed(123)
training_split = 0.75 #75% data for train, 25% for #test
train_dat = dat %>% sample_frac(training_split)
test_dat = dat %>% anti_join(train_dat, by = 'PassengerId')
```



```{r}
mod1 = glm(Survived ~ ., family = binomial(link = 'logit'), data = train_dat)
summary(mod1)
```

You notice that some of the values are insignificant. Remove *one* that seems the least likely to impact `Survived`
```{r}
mod2 = glm(Survived ~ . - Embarked, family = binomial(link = 'logit'), data = train_dat)
summary(mod2)
```


```{r}
mod3 = glm(Survived ~ . - Embarked - Parch, family = binomial(link = 'logit'), data = train_dat)
summary(mod3)
```


```{r}
mod4 = glm(Survived ~ . - Embarked - Parch - Fare, family = binomial(link = 'logit'), data = train_dat)
summary(mod4)
```


## Test your model's accuracy
```{r}
cutoff_level = 0.5

results = train_dat %>%
  add_predictions(model = mod4, var = 'pred', type = 'response') %>%
  mutate(pred_binary = factor(if_else(pred <= cutoff_level, 0, 1)))
head(results)
```


```{r}
cm = confusionMatrix(data=results$pred_binary, reference=results$Survived)
cm
```


## How does it compare to null error rate?

```{r}
# Null error rate is defined as always guessing the higher probability (in this case, did not survive)
train_survived_false = train_dat %>% filter(Survived == 0)  %>% nrow()
null_error_rate = train_survived_false / nrow(train_dat)

print(paste0('Accuracy: ', 100*round(cm$overall[1], 2), '%'))
print(paste0('Null Error Rate: ', 100*round(null_error_rate, 2), '%'))
```
#FB: Our rate is 17% higher than the Null Error Rate.

## Extract confusion matrix data
Find true positive, false positive, true negative, false negative. Also find `tp_rate` and `fp_rate`
```{r}
conf_mat = cm$table
conf_mat

```
```{r}
conf_mat = cm$table

tp = conf_mat[2,2]
tn = conf_mat[1,1]
fn = conf_mat[2,1]
fp = conf_mat[1,2]

#tp_rate =  # aka "sensitivity" or "recall" when yes, how often does it predict yes?
tp_rate = (tp/(tp + fn))


#fp_rate =  # aka "specificity" when no, how often does it predict yes?tp_rate = tp/(tp + fn)
fp_rate = (fp/(fp + tn))
tp_rate
fp_rate
```


# Create a function to make the "best" model by tuning the cutoff parameter
```{r}
model_cutoff = function(df, mod, cutoff_level){ 

results = df %>%
  add_predictions(model = mod, var = 'pred', type = 'response') %>%
  mutate(pred_binary = factor(if_else(pred <= cutoff_level, 0, 1)))
#head(results)
conf_mat = cm$table

tp = conf_mat[2,2]
tn = conf_mat[1,1]
fn = conf_mat[2,1]
fp = conf_mat[1,2]
tp_rate = tp/(tp + fn)
fp_rate = fp/(fp + tn)

#
print(cutoff_level)
return(tp_rate)
return(fp_rate)

}
```

```{r}

model_cutoff(results, mod4, .5)

```

Your function should output:
cutoff_level 0.5
tp_rate 0.76666666...
fp_rate 0.1869...

#FB: Mine is off a bit...but matches the "answers" version!!!!
```{r}
train_dat %>% model_cutoff(mod = mod4, cutoff_level = 0.5)
```

Iterate through function to create a tibble of true positive rates, false positive rates for cutoff levels from 0 to 1 by 0.01 (0, 0.01, 0.02, ..., 1)
```{r}

for(i in 1:100) {

  cutoff_level = 1/i
  tmp = train_dat 
  my_results = model_cutoff(results, mod4, cutoff_level) #%>%
    
  #bind(tmp)
}
  
#FB: Had to removed the bind line to work at all!
```


Create ROC curve and plot your data points
```{r}
plot(my_results$fp_rate, my_results$tp_rate)
```

Pick a cutoff level and defend your selection.
